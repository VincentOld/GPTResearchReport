
<!DOCTYPE HTML>
<html lang="" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>Windows+6GB显+CPU本地部署ChatGLM-6B.md · GitBook</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.2.3">
        
        
        
    
    <link rel="stylesheet" href="../gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="../gitbook/images/favicon.ico" type="image/x-icon">

    
    
    <link rel="prev" href="训练部署自己的ChatGLM-6B.html" />
    

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="Type to search" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="../">
            
                <a href="../">
            
                    
                    Introduction
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" data-path="../天工GPT调研/天工GPT调研.html">
            
                <a href="../天工GPT调研/天工GPT调研.html">
            
                    
                    天工GPT调研
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3" data-path="../相关GPT调研/相关GPT调研.html">
            
                <a href="../相关GPT调研/相关GPT调研.html">
            
                    
                    相关GPT调研
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4" >
            
                <span>
            
                    
                    相关技术调研
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.4.1" >
            
                <span>
            
                    
                    第一章：训练微调相关技术
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.4.1.1" data-path="../Chapter2/微调范式对比.html">
            
                <a href="../Chapter2/微调范式对比.html">
            
                    
                    节省显存的微调推理技术对比
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.1.2" data-path="../Chapter2/大模型微调技术：fine-tune、parameter-efficient fine-tune和prompt-tune.html">
            
                <a href="../Chapter2/大模型微调技术：fine-tune、parameter-efficient fine-tune和prompt-tune.html">
            
                    
                    LLM三种微调技术对比
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.1.3" data-path="../Chapter2/Prompt-Tuning、Instruction-Tuning和Chain-of-Thought.html">
            
                <a href="../Chapter2/Prompt-Tuning、Instruction-Tuning和Chain-of-Thought.html">
            
                    
                    LLM三种训练技术对比
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.1.4" data-path="../Chapter2/思维链.html">
            
                <a href="../Chapter2/思维链.html">
            
                    
                    思维链简介
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.4.2" >
            
                <span>
            
                    
                    第二章：定制化GPT
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.4.2.1" data-path="根据自己数据库让GPT作答.html">
            
                <a href="根据自己数据库让GPT作答.html">
            
                    
                    根据自己数据库让GPT作答
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.2.2" data-path="训练部署自己的羊驼 Alpaca-LoRA.html">
            
                <a href="训练部署自己的羊驼 Alpaca-LoRA.html">
            
                    
                    训练部署自己的羊驼 Alpaca-LoRA
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.2.3" data-path="训练部署自己的ChatGLM-6B.html">
            
                <a href="训练部署自己的ChatGLM-6B.html">
            
                    
                    训练部署自己的ChatGLM-6B
            
                </a>
            

            
        </li>
    
        <li class="chapter active" data-level="1.4.2.4" data-path="Windows+6GB显卡版本和CPU版本的本地部署ChatGLM-6B.html">
            
                <a href="Windows+6GB显卡版本和CPU版本的本地部署ChatGLM-6B.html">
            
                    
                    Windows+6GB显+CPU本地部署ChatGLM-6B.md
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.4.3" >
            
                <span>
            
                    
                    第三章：数据底座方法与调研
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.4.3.1" data-path="../Chapter3/Self-Instruct数据.html">
            
                <a href="../Chapter3/Self-Instruct数据.html">
            
                    
                    Self-Instruct自动生成微调数据
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.3.2" data-path="../Chapter3/学术问答数据集调研.html">
            
                <a href="../Chapter3/学术问答数据集调研.html">
            
                    
                    学术问答数据集调研
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.4.4" >
            
                <span>
            
                    
                    第四章：其他相关知识
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.4.4.1" data-path="../Chapter4/Lamini.html">
            
                <a href="../Chapter4/Lamini.html">
            
                    
                    Lamini LLM引擎学习
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.4.2" data-path="../Chapter4/GPT与知识图谱.html">
            
                <a href="../Chapter4/GPT与知识图谱.html">
            
                    
                    GPT与知识图谱
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.4.3" data-path="../Chapter4/Entailment Trees论文学习笔记.html">
            
                <a href="../Chapter4/Entailment Trees论文学习笔记.html">
            
                    
                    Entailment Trees论文学习笔记
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.4.4" data-path="../Chapter4/METGEN论文学习笔记.html">
            
                <a href="../Chapter4/METGEN论文学习笔记.html">
            
                    
                    METGEN论文学习笔记
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.5" data-path="../技术框架/技术框架.html">
            
                <a href="../技术框架/技术框架.html">
            
                    
                    毕设技术框架
            
                </a>
            

            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            Published with GitBook
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href=".." >Windows+6GB显+CPU本地部署ChatGLM-6B.md</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <h1 id="&#x8BAD;&#x7EC3;&#x90E8;&#x7F72;&#x81EA;&#x5DF1;&#x7684;chatglm-6b">&#x8BAD;&#x7EC3;&#x90E8;&#x7F72;&#x81EA;&#x5DF1;&#x7684;ChatGLM-6B</h1>
<p>&#x53C2;&#x8003;&#x94FE;&#x63A5;&#xFF1A;<a href="https://mp.weixin.qq.com/s/8gdrmdZfQO_Ji_frXpjc7g" target="_blank">&#x624B;&#x628A;&#x624B;&#x6559;&#x4F60;&#x672C;&#x5730;&#x90E8;&#x7F72;&#x6E05;&#x534E;&#x5927;&#x5B66;KEG&#x7684;ChatGLM-6B&#x6A21;&#x578B;&#x2014;&#x2014;Windows+6GB&#x663E;&#x5361;&#x7248;&#x672C;&#x548C;CPU&#x7248;&#x672C;&#x7684;&#x672C;&#x5730;&#x90E8;&#x7F72; </a></p>
<pre><code>&#x5B89;&#x88C5;&#x524D;&#x8BF4;&#x660E;
&#x90E8;&#x7F72;&#x524D;&#x5B89;&#x88C5;&#x73AF;&#x5883;
    1&#x3001;&#x4E0B;&#x8F7D;&#x5B98;&#x65B9;&#x4EE3;&#x7801;&#xFF0C;&#x5B89;&#x88C5;Python&#x4F9D;&#x8D56;&#x7684;&#x5E93;
    2&#x3001;&#x4E0B;&#x8F7D;INT4&#x91CF;&#x5316;&#x540E;&#x7684;&#x9884;&#x8BAD;&#x7EC3;&#x7ED3;&#x679C;&#x6587;&#x4EF6;
Windows+GPU&#x90E8;&#x7F72;&#x65B9;&#x6848;
    1&#x3001;Windows+GPU&#x65B9;&#x6848;&#x7684;&#x5FC5;&#x5907;&#x6761;&#x4EF6;
    2&#x3001;&#x8FD0;&#x884C;&#x90E8;&#x7F72;GPU&#x7248;&#x672C;&#x7684;INT4&#x91CF;&#x5316;&#x7684;ChatGLM-6B&#x6A21;&#x578B;
Windows+CPU&#x90E8;&#x7F72;&#x65B9;&#x6848;
    1&#x3001;Windows+CPU&#x65B9;&#x6848;&#x7684;&#x5FC5;&#x5907;&#x6761;&#x4EF6;
    2&#x3001;&#x8FD0;&#x884C;&#x90E8;&#x7F72;CPU&#x7248;&#x672C;&#x7684;INT4&#x91CF;&#x5316;&#x7684;ChatGLM-6B&#x6A21;&#x578B;
&#x603B;&#x7ED3;
</code></pre><hr>
<h4 id="&#x4E00;&#x3001;windowsgpu6g">&#x4E00;&#x3001;windows+GPU6G</h4>
<ol>
<li><h5 id="&#x90E8;&#x7F72;&#x524D;&#x5B89;&#x88C5;&#x73AF;&#x5883;">&#x90E8;&#x7F72;&#x524D;&#x5B89;&#x88C5;&#x73AF;&#x5883;</h5>
</li>
</ol>
<pre><code class="lang-shell">&#x90E8;&#x7F72;&#x524D;&#x5B89;&#x88C5;&#x73AF;&#x5883;
&#x9996;&#x5148;&#xFF0C;&#x6211;&#x4EEC;&#x9700;&#x8981;&#x4ECE;GitHub&#x4E0A;&#x4E0B;&#x8F7D;ChatGLM&#x7684;requirements.txt&#x6765;&#x5E2E;&#x52A9;&#x6211;&#x4EEC;&#x5B89;&#x88C5;&#x4F9D;&#x8D56;&#x7684;&#x5E93;&#x3002;
&#x5728;GitHub&#x4E0A;&#x4E0B;&#x8F7D;requirements.txt&#x5373;&#x53EF;&#x3002;&#x4E0B;&#x8F7D;&#x5730;&#x5740;&#xFF1A;https://github.com/THUDM/ChatGLM-6B

&#x5B89;&#x88C5;&#x4F9D;&#x8D56;&#xFF1A;
pip install -r requirements.txt
&#x5176;&#x4E2D; transformers &#x5E93;&#x7248;&#x672C;&#x5FC5;&#x987B;&#x662F;4.27.1&#x53CA;&#x4EE5;&#x4E0A;&#x7684;&#x7248;&#x672C;&#x624D;&#x53EF;&#x4EE5;

&#x53E6;&#x5916;&#xFF0C;ChatGLM-6B&#x4F9D;&#x8D56;torch&#xFF0C;&#x5982;&#x679C;&#x4F60;&#x6709;GPU&#xFF0C;&#x4E14;&#x9AD8;&#x4E8E;6G&#x5185;&#x5B58;&#xFF0C;&#x90A3;&#x4E48;&#x5EFA;&#x8BAE;&#x90E8;&#x7F72;GPU&#x7248;&#x672C;&#xFF0C;
&#x4F46;&#x662F;&#x9700;&#x8981;&#x4E0B;&#x8F7D;&#x652F;&#x6301;cuda&#x7684;torch&#xFF0C;&#x800C;&#x4E0D;&#x662F;&#x9ED8;&#x8BA4;&#x7684;CPU&#x7248;&#x672C;&#x7684;torch&#x3002;
</code></pre>
<h5 id="2-&#x4E0B;&#x8F7D;int4&#x91CF;&#x5316;&#x540E;&#x7684;&#x9884;&#x8BAD;&#x7EC3;&#x7ED3;&#x679C;&#x6587;&#x4EF6;">2 &#x4E0B;&#x8F7D;INT4&#x91CF;&#x5316;&#x540E;&#x7684;&#x9884;&#x8BAD;&#x7EC3;&#x7ED3;&#x679C;&#x6587;&#x4EF6;</h5>
<pre><code class="lang-shell">INT4&#x91CF;&#x5316;&#x7684;&#x9884;&#x8BAD;&#x7EC3;&#x6587;&#x4EF6;&#x4E0B;&#x8F7D;&#x5730;&#x5740;&#xFF1A;https://huggingface.co/THUDM/chatglm-6b-int4/tree/main

&#x9700;&#x8981;&#x6CE8;&#x610F;&#x7684;&#x662F;&#xFF0C;&#x5728;GitHub&#x4E0A;&#xFF0C;&#x5B98;&#x65B9;&#x63D0;&#x4F9B;&#x4E86;&#x6A21;&#x578B;&#x5728;&#x6E05;&#x534E;&#x4E91;&#x4E0A;&#x7684;&#x4E0B;&#x8F7D;&#x5730;&#x5740;&#xFF0C;&#x4F46;&#x662F;&#x90A3;&#x4E2A;&#x53EA;&#x5305;&#x542B;&#x9884;&#x8BAD;&#x7EC3;&#x7ED3;&#x679C;&#x6587;&#x4EF6;&#x5373;bin&#x6587;&#x4EF6;&#xFF0C;
&#x4F46;&#x5B9E;&#x9645;&#x4E0A;ChatGLM-6B&#x7684;&#x8FD0;&#x884C;&#x9700;&#x8981;&#x6A21;&#x578B;&#x7684;&#x914D;&#x7F6E;&#x6587;&#x4EF6;&#xFF0C;&#x5373;config.json&#x7B49;&#x3002;
&#x56E0;&#x6B64;&#x5EFA;&#x8BAE;&#x5168;&#x90E8;&#x4ECE;HuggingFace&#x4E0A;&#x4E0B;&#x8F7D;&#x6240;&#x6709;&#x6587;&#x4EF6;&#x5230;&#x672C;&#x5730;&#x3002;
&#x4E0A;&#x8FF0;&#x6587;&#x4EF6;&#x5168;&#x90E8;&#x4E0B;&#x8F7D;&#x4E4B;&#x540E;&#x4FDD;&#x5B58;&#x5230;&#x672C;&#x5730;&#x7684;&#x4E00;&#x4E2A;&#x76EE;&#x5F55;&#x4E0B;&#x5373;&#x53EF;&#xFF0C;&#x6211;&#x4EEC;&#x4FDD;&#x5B58;&#x5728;&#xFF1A;D:\\chatglm-6b-int4
</code></pre>
<h5 id="3-windowsgpu&#x90E8;&#x7F72;&#x65B9;&#x6848;">3 Windows+GPU&#x90E8;&#x7F72;&#x65B9;&#x6848;</h5>
<p>&#x5BF9;&#x4E8E;ChatGLM-6B&#x6A21;&#x578B;&#x7684;&#x8BAD;&#x7EC3;&#xFF0C;&#x9700;&#x8981;&#x51C6;&#x5907;&#x76F8;&#x5E94;&#x7684;&#x6570;&#x636E;&#x96C6;&#x3002;&#x4F7F;&#x7528;ADGEN&#x6570;&#x636E;&#x96C6;&#xFF0C;&#x5176;&#x4EFB;&#x52A1;&#x4E3A;&#x6839;&#x636E;&#x8F93;&#x5165;&#xFF08;content&#xFF09;&#x751F;&#x6210;&#x4E00;&#x6BB5;&#x5E7F;&#x544A;&#x8BCD;&#xFF08;summary&#xFF09;&#x3002;&#x4E0B;&#x8F7D;ADGEN&#x6570;&#x636E;&#x96C6;&#xFF0C;&#x4ECE; Google Drive &#x6216;&#x8005; Tsinghua Cloud &#x4E0B;&#x8F7D;&#x5904;&#x7406;&#x597D;&#x7684; ADGEN &#x6570;&#x636E;&#x96C6;&#xFF0C;&#x5C06;&#x89E3;&#x538B;&#x540E;&#x7684; AdvertiseGen &#x76EE;&#x5F55;&#x653E;&#x5230;&#x672C;&#x76EE;&#x5F55;&#x4E0B;&#x3002;</p>
<pre><code class="lang-shell">&#x90E8;&#x7F72;GPU&#x7248;&#x672C;&#x7684;ChatGLM-6B&#x9700;&#x8981;&#x5B89;&#x88C5;cuda&#x7248;&#x672C;&#x7684;torch&#xFF0C;
&#x9700;&#x8981;&#x68C0;&#x6D4B;&#x81EA;&#x5DF1;&#x7684;torch&#x662F;&#x5426;&#x6B63;&#x786E;&#xFF0C;&#x53EF;&#x4EE5;&#x901A;&#x8FC7;&#x5982;&#x4E0B;&#x547D;&#x4EE4;&#x68C0;&#x67E5;&#xFF08;&#x4E0B;&#x9762;&#x662F;python&#x4EE3;&#x7801;&#xFF09;&#xFF1A;
import torch
print(torch.cuda.is_available())
&#x5982;&#x679C;&#x4EE5;&#x4E0A;&#x4EE3;&#x7801;&#x8F93;&#x51FA;&#x7684;&#x662F;True&#xFF0C;&#x90A3;&#x4E48;&#x606D;&#x559C;&#x4F60;&#xFF0C;&#x4F60;&#x5B89;&#x88C5;&#x7684;&#x662F;cuda&#x7248;&#x672C;&#x7684;torch
&#xFF08;&#x6CE8;&#x610F;&#xFF0C;&#x6709;&#x663E;&#x5361;&#x4E5F;&#x9700;&#x8981;&#x4E0B;&#x8F7D;cuda&#x548C;cudann&#x5B89;&#x88C5;&#x6210;&#x529F;&#x624D;&#x53EF;&#x4EE5;&#xFF0C;&#x8FD9;&#x90E8;&#x5206;&#x53EF;&#x4EE5;&#x53BB;&#x7F51;&#x4E0A;&#x627E;&#x6559;&#x7A0B;&#xFF09;&#x3002;

&#x6CE8;&#x610F;&#xFF0C;&#x76EE;&#x524D;ChatGLM-6B&#x6709;3&#x4E2A;&#x7248;&#x672C;&#x53EF;&#x4EE5;&#x4F7F;&#x7528;&#xFF0C;
&#x6CA1;&#x6709;&#x91CF;&#x5316;&#x7684;&#x7248;&#x672C;&#x505A;&#x63A8;&#x7406;&#x9700;&#x8981;13G&#x7684;GPU&#x663E;&#x5B58;&#xFF0C;INT8&#x91CF;&#x5316;&#x9700;&#x8981;8GB&#x7684;&#x663E;&#x5B58;&#xFF0C;&#x800C;INT4&#x91CF;&#x5316;&#x7684;&#x7248;&#x672C;&#x9700;&#x8981;6GB&#x7684;&#x663E;&#x5B58;&#x3002;
&#x6CE8;&#x610F;&#xFF1A;&#x6A21;&#x578B;&#x91CF;&#x5316;&#x4F1A;&#x5E26;&#x6765;&#x4E00;&#x5B9A;&#x7684;&#x6027;&#x80FD;&#x635F;&#x5931;&#xFF0C;&#x7ECF;&#x8FC7;&#x6D4B;&#x8BD5;&#xFF0C;ChatGLM-6B &#x5728; 4-bit &#x91CF;&#x5316;&#x4E0B;&#x4ECD;&#x7136;&#x80FD;&#x591F;&#x8FDB;&#x884C;&#x81EA;&#x7136;&#x6D41;&#x7545;&#x7684;&#x751F;&#x6210;&#x3002;
</code></pre>
<h5 id="4-&#x8FD0;&#x884C;&#x90E8;&#x7F72;gpu&#x7248;&#x672C;&#x7684;int4&#x91CF;&#x5316;&#x7684;chatglm-6b&#x6A21;&#x578B;">4 &#x8FD0;&#x884C;&#x90E8;&#x7F72;GPU&#x7248;&#x672C;&#x7684;INT4&#x91CF;&#x5316;&#x7684;ChatGLM-6B&#x6A21;&#x578B;</h5>
<pre><code class="lang-shell">GPU&#x7248;&#x672C;&#x7684;&#x6A21;&#x578B;&#x90E8;&#x7F72;&#x5F88;&#x7B80;&#x5355;&#xFF0C;&#x4E0A;&#x8FF0;&#x4E24;&#x4E2A;&#x6B65;&#x9AA4;&#x5B8C;&#x6210;&#x4E4B;&#x540E;&#x5373;&#x53EF;&#x8FD0;&#x884C;&#x3002;&#x4EE3;&#x7801;&#x5982;&#x4E0B;&#xFF1A;
from transformers importAutoTokenizer,AutoModel
tokenizer = AutoTokenizer.from_pretrained(&quot;D:\\data\\llm\\chatglm-6b-int4&quot;, trust_remote_code=True, revision=&quot;&quot;)
model =AutoModel.from_pretrained(&quot;D:\\data\\llm\\chatglm-6b-int4&quot;, trust_remote_code=True, revision=&quot;&quot;).half().cuda()
model = model.eval()
response, history = model.chat(tokenizer,&quot;&#x4F60;&#x597D;&quot;, history=[])
print(response)
</code></pre>
<p>&#x901A;&#x8FC7;&#x4EE5;&#x4E0A;&#x6B65;&#x9AA4;&#x6211;&#x4EEC;&#x53EF;&#x4EE5;&#x5F97;&#x5230;&#x5982;&#x4E0B;&#x7ED3;&#x679C;&#xFF1A;GPU&#x7248;&#x672C;&#x5927;&#x7EA6;&#x53EA;&#x9700;&#x8981;1-2&#x79D2;&#x5373;&#x53EF;&#x83B7;&#x5F97;&#x7ED3;&#x679C;</p>
<p><img src="pics/Windows+6GB&#x663E;&#x5361;&#x7248;&#x672C;&#x548C;CPU&#x7248;&#x672C;&#x7684;&#x672C;&#x5730;&#x90E8;&#x7F72;ChatGLM-6B/image-20230506171345898.png" alt="image-20230506171345898" style="zoom:50%;"> </p>
<hr>
<h4 id="&#x4E8C;&#x3001;windowscpu">&#x4E8C;&#x3001;windows+CPU</h4>
<h5 id="1-gcc&#x7F16;&#x8BD1;&#x73AF;&#x5883;&#x914D;&#x7F6E;kernel&#x7F16;&#x8BD1;">1. GCC&#x7F16;&#x8BD1;&#x73AF;&#x5883;&#x914D;&#x7F6E;+kernel&#x7F16;&#x8BD1;</h5>
<pre><code class="lang-shell">CPU&#x7248;&#x672C;&#x7684;ChatGLM-6B&#x90E8;&#x7F72;&#x6BD4;GPU&#x7248;&#x672C;&#x7A0D;&#x5FAE;&#x9EBB;&#x70E6;&#x4E00;&#x70B9;&#xFF0C;&#x4E3B;&#x8981;&#x6D89;&#x53CA;&#x5230;&#x4E00;&#x4E2A;kernel&#x7684;&#x7F16;&#x8BD1;&#x95EE;&#x9898;&#x3002;
&#x5728;&#x5B89;&#x88C5;&#x4E4B;&#x524D;&#xFF0C;&#x9664;&#x4E86;&#x4E0A;&#x9762;&#x9700;&#x8981;&#x5B89;&#x88C5;&#x597D;requirements.txt&#x4E2D;&#x6240;&#x6709;&#x7684;Python&#x4F9D;&#x8D56;&#x5916;&#xFF0C;torch&#x9700;&#x8981;&#x5B89;&#x88C5;&#x597D;&#x6B63;&#x5E38;&#x7684;CPU&#x7248;&#x672C;&#x5373;&#x53EF;&#x3002;
&#x4F46;&#x662F;&#xFF0C;&#x9664;&#x4E86;&#x8FD9;&#x4E9B;CPU&#x7248;&#x672C;&#x7684;&#x5B89;&#x88C5;&#x8FD8;&#x9700;&#x8981;&#x5728;&#x672C;&#x5730;&#x7684;Windows&#x4E0B;&#x5B89;&#x88C5;&#x597D;C/C++&#x7684;&#x7F16;&#x8BD1;&#x73AF;&#x5883;&#x3002;&#x63A8;&#x8350;&#x5B89;&#x88C5;TDM-GCC&#xFF0C;&#x4E0B;&#x8F7D;&#x5730;&#x5740;&#xFF1A;https://jmeubank.github.io/tdm-gcc/
&#x76F4;&#x63A5;&#x70B9;&#x51FB;&#x4E0A;&#x8FF0;&#x9875;&#x9762;&#x4E2D;TDM-GCC 10.3.0 release&#x4E0B;&#x8F7D;&#x5B89;&#x88C5;&#x5373;&#x53EF;&#xFF0C;&#x6CE8;&#x610F;&#x5B89;&#x88C5;&#x7684;&#x65F6;&#x5019;&#x76F4;&#x63A5;&#x9009;&#x62E9;&#x5168;&#x90E8;&#x5B89;&#x88C5;&#x5C31;&#x597D;&#x3002;
&#x5B89;&#x88C5;&#x5B8C;&#x5728;cmd&#x4E2D;&#x8FD0;&#x884C;&#x201D;gcc -v&#x201D;&#x6D4B;&#x8BD5;&#x662F;&#x5426;&#x6210;&#x529F;&#x5373;&#x53EF;&#x3002;
</code></pre>
<p><img src="pics/Windows+6GB&#x663E;&#x5361;&#x7248;&#x672C;&#x548C;CPU&#x7248;&#x672C;&#x7684;&#x672C;&#x5730;&#x90E8;&#x7F72;ChatGLM-6B/image-20230506171535510.png" alt="image-20230506171535510" style="zoom:43%;"> </p>
<p>&#x6CE8;&#xFF1A;&#x5B89;&#x88C5;&#x8FD9;&#x4E2A;&#x4E3B;&#x8981;&#x662F;&#x4E3A;&#x4E86;&#x7F16;&#x8BD1;&#x4E4B;&#x524D;&#x4E0B;&#x8F7D;&#x7684;&#x6587;&#x4EF6;&#x4E2D;&#x7684;<code>quantization_kernels.c</code>&#x548C;<code>quantization_kernels_parallel.c</code>&#x8FD9;&#x4E24;&#x4E2A;&#x6587;&#x4EF6;&#x3002;
&#x5982;&#x679C;&#x5728;&#x8FD0;&#x884C;&#x4E2D;&#x9047;&#x5230;&#x4E86;&#x5982;&#x4E0B;&#x9519;&#x8BEF;&#x63D0;&#x793A;&#xFF1A;</p>
<pre><code class="lang-shell">No compiled kernel found.
Compiling kernels : C:\Users\DuFei\.cache\huggingface\modules\transformers_modules\chatglm-6b-int4\quantization_kernels_parallel.c
Compiling gcc -O3 -fPIC -pthread -fopenmp -std=c99 C:\Users\DuFei\.cache\huggingface\modules\transformers_modules\chatglm-6b-int4\quantization_kernels_parallel.c -shared -o C:\Users\DuFei\.cache\huggingface\modules\transformers_modules\chatglm-6b-int4\quantization_kernels_parallel.so
Kernels compiled : C:\Users\DuFei\.cache\huggingface\modules\transformers_modules\chatglm-6b-int4\quantization_kernels_parallel.so
Cannot load cpu kernel, don&apos;t use quantized model on cpu.
Using quantization cache
Applying quantization to glm layers
</code></pre>
<p>&#x90A3;&#x4E48;&#x5C31;&#x662F;&#x8FD9;&#x4E24;&#x4E2A;&#x6587;&#x4EF6;&#x7F16;&#x8BD1;&#x51FA;&#x95EE;&#x9898;&#x4E86;&#x3002;&#x90A3;&#x4E48;&#x5C31;&#x9700;&#x8981;&#x6211;&#x4EEC;&#x624B;&#x52A8;&#x53BB;&#x7F16;&#x8BD1;&#x8FD9;&#x4E24;&#x4E2A;&#x6587;&#x4EF6;&#xFF1A;</p>
<p>&#x5373;&#x5728;&#x4E0A;&#x9762;&#x4E0B;&#x8F7D;&#x7684;<code>D:\\data\\llm\\chatglm-6b-int4</code>&#x672C;&#x5730;&#x76EE;&#x5F55;&#x4E0B;&#x8FDB;&#x5165;cmd&#xFF0C;&#x8FD0;&#x884C;&#x5982;&#x4E0B;&#x4E24;&#x4E2A;&#x7F16;&#x8BD1;&#x547D;&#x4EE4;&#xFF1A;</p>
<pre><code class="lang-shell">gcc -fPIC -pthread -fopenmp -std=c99 quantization_kernels.c -shared -o quantization_kernels.so
gcc -fPIC -pthread -fopenmp -std=c99 quantization_kernels_parallel.c -shared -o quantization_kernels_parallel.so
</code></pre>
<p><img src="pics/Windows+6GB&#x663E;&#x5361;&#x7248;&#x672C;&#x548C;CPU&#x7248;&#x672C;&#x7684;&#x672C;&#x5730;&#x90E8;&#x7F72;ChatGLM-6B/image-20230506171651317.png" alt="image-20230506171651317" style="zoom:50%;"> </p>
<p>&#x7136;&#x540E;&#x5C31;&#x53EF;&#x4EE5;&#x5728;<code>D:\\data\\llm\\chatglm-6b-int4</code>&#x76EE;&#x5F55;&#x4E0B;&#x770B;&#x5230;&#x4E0B;&#x9762;&#x4E24;&#x4E2A;&#x65B0;&#x7684;&#x6587;&#x4EF6;&#xFF1A;</p>
<p><code>quantization_kernels_parallel.so</code>&#x548C;<code>quantization_kernels.so</code>&#x3002;&#x8BF4;&#x660E;&#x7F16;&#x8BD1;&#x6210;&#x529F;&#xFF0C;&#x540E;&#x9762;&#x6211;&#x4EEC;&#x624B;&#x52A8;&#x8F7D;&#x5165;&#x5373;&#x53EF;&#x3002;</p>
<h5 id="2-&#x8FD0;&#x884C;&#x90E8;&#x7F72;cpu&#x7248;&#x672C;&#x7684;int4&#x91CF;&#x5316;&#x7684;chatglm-6b&#x6A21;&#x578B;">2. &#x8FD0;&#x884C;&#x90E8;&#x7F72;CPU&#x7248;&#x672C;&#x7684;INT4&#x91CF;&#x5316;&#x7684;ChatGLM-6B&#x6A21;&#x578B;</h5>
<pre><code class="lang-shell">from transformers importAutoTokenizer,AutoModel
tokenizer =AutoTokenizer.from_pretrained(&quot;D:\\data\\llm\\chatglm-6b-int4&quot;, trust_remote_code=True, revision=&quot;&quot;)
model =AutoModel.from_pretrained(&quot;D:\\data\\llm\\chatglm-6b-int4&quot;,trust_remote_code=True, revision=&quot;&quot;).float()
model = model.eval()
response, history = model.chat(tokenizer,&quot;&#x4F60;&#x597D;&quot;, history=[])
print(response)
</code></pre>
<p>&#x6CE8;&#x610F;&#xFF0C;&#x5176;&#x5B9E;&#x5C31;&#x662F;&#x7B2C;&#x4E09;&#x884C;&#x4EE3;&#x7801;&#x6700;&#x540E;&#x7684;<code>float()</code>&#x6709;&#x5DEE;&#x5F02;&#xFF1A;GPU&#x7248;&#x672C;&#x540E;&#x9762;&#x662F;<code>.half().cuda()</code>&#xFF0C;&#x800C;&#x8FD9;&#x91CC;&#x662F;<code>float()</code>&#x3002;</p>
<pre><code class="lang-shell">model =AutoModel.from_pretrained(&quot;D:\\data\\llm\\chatglm-6b-int4&quot;, trust_remote_code=True, revision=&quot;&quot;).float()
</code></pre>
<h5 id="3-&#x5982;&#x679C;&#x4F60;&#x8FD0;&#x884C;&#x4E0A;&#x9762;&#x7684;&#x4EE3;&#x7801;&#x51FA;&#x73B0;&#x5982;&#x4E0B;&#x9519;&#x8BEF;">3. &#x5982;&#x679C;&#x4F60;&#x8FD0;&#x884C;&#x4E0A;&#x9762;&#x7684;&#x4EE3;&#x7801;&#x51FA;&#x73B0;&#x5982;&#x4E0B;&#x9519;&#x8BEF;</h5>
<pre><code>AttributeError:&apos;NoneType&apos;object has no attribute &apos;int4WeightExtractionFloat&apos;
</code></pre><p>&#x90A3;&#x4E48;&#x5C31;&#x662F;&#x524D;&#x9762;&#x8BF4;&#x7684;&#x7F16;&#x8BD1;&#x6587;&#x4EF6;&#x51FA;&#x4E86;&#x95EE;&#x9898;&#xFF0C;&#x90A3;&#x4E48;&#x5C31;&#x5FC5;&#x987B;&#x505A;&#x4E0A;&#x9762;&#x8BF4;&#x7684;&#x7F16;&#x8BD1;&#x64CD;&#x4F5C;&#xFF0C;&#x5F97;&#x5230;&#x90A3;2&#x4E2A;so&#x6587;&#x4EF6;&#xFF0C;&#x7136;&#x540E;&#x624B;&#x52A8;&#x52A0;&#x8F7D;&#x3002;&#x65B0;&#x4EE3;&#x7801;&#x5982;&#x4E0B;&#xFF1A;</p>
<pre><code class="lang-shell">from transformers importAutoTokenizer,AutoModel
tokenizer =AutoTokenizer.from_pretrained(&quot;D:\\data\\llm\\chatglm-6b-int4&quot;, trust_remote_code=True, revision=&quot;&quot;)
model =AutoModel.from_pretrained(&quot;D:\\data\\llm\\chatglm-6b-int4&quot;,trust_remote_code=True, revision=&quot;&quot;).float()
model = model.quantize(bits=4, kernel_file=&quot;D:\\data\\llm\\chatglm-6b-int4\\quantization_kernels.so&quot;)
model = model.eval()
response, history = model.chat(tokenizer,&quot;&#x4F60;&#x597D;&quot;, history=[])
print(response)
</code></pre>
<p>&#x6BD4;&#x539F;&#x6765;&#x7684;&#x4EE3;&#x7801;&#x591A;&#x4E86;<code>model = model.quantize(bits=4, kernel_file=&quot;D:\\data\\llm\\chatglm-6b-int4\\quantization_kernels.so&quot;)</code>&#x4E00;&#x884C;&#x624B;&#x52A8;&#x52A0;&#x8F7D;&#x7684;&#x5185;&#x5BB9;&#x3002;</p>
<p><img src="pics/Windows+6GB&#x663E;&#x5361;&#x7248;&#x672C;&#x548C;CPU&#x7248;&#x672C;&#x7684;&#x672C;&#x5730;&#x90E8;&#x7F72;ChatGLM-6B/image-20230506171930862.png" alt="image-20230506171930862" style="zoom:50%;"> </p>
<p>&#x6CE8;&#xFF1A;CPU&#x7248;&#x672C;&#x7684;&#x6A21;&#x578B;&#x63A8;&#x7406;&#x8FD0;&#x884C;&#x4E00;&#x6B21;&#x7EA6;1-2&#x5206;&#x949F;&#xFF0C;&#x5B9E;&#x5728;&#x662F;&#x592A;&#x6162;&#x4E86;&#xFF0C;&#x57FA;&#x672C;&#x4E0D;&#x9002;&#x5408;&#x4F7F;&#x7528;&#x3002;&#x6709;&#x673A;&#x4F1A;&#x8FD8;&#x662F;&#x641E;GPU&#x7248;&#x672C;&#x5427;&#xFF01;</p>

                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="训练部署自己的ChatGLM-6B.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page: 训练部署自己的ChatGLM-6B">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"Windows+6GB显+CPU本地部署ChatGLM-6B.md","level":"1.4.2.4","depth":3,"next":{"title":"第三章：数据底座方法与调研","level":"1.4.3","depth":2,"ref":"","articles":[{"title":"Self-Instruct自动生成微调数据","level":"1.4.3.1","depth":3,"path":"Chapter3/Self-Instruct数据.md","ref":"Chapter3/Self-Instruct数据.md","articles":[]},{"title":"学术问答数据集调研","level":"1.4.3.2","depth":3,"path":"Chapter3/学术问答数据集调研.md","ref":"Chapter3/学术问答数据集调研.md","articles":[]}]},"previous":{"title":"训练部署自己的ChatGLM-6B","level":"1.4.2.3","depth":3,"path":"Chapter1/训练部署自己的ChatGLM-6B.md","ref":"Chapter1/训练部署自己的ChatGLM-6B.md","articles":[]},"dir":"ltr"},"config":{"gitbook":"*","theme":"default","variables":{},"plugins":["livereload"],"pluginsConfig":{"livereload":{},"highlight":{},"search":{},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"sharing":{"facebook":true,"twitter":true,"google":false,"weibo":false,"instapaper":false,"vk":false,"all":["facebook","google","twitter","weibo","instapaper"]},"fontsettings":{"theme":"white","family":"sans","size":2},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"}},"file":{"path":"Chapter1/Windows+6GB显卡版本和CPU版本的本地部署ChatGLM-6B.md","mtime":"2023-05-06T09:20:31.521Z","type":"markdown"},"gitbook":{"version":"3.2.3","time":"2023-06-01T03:46:06.651Z"},"basePath":"..","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="../gitbook/gitbook.js"></script>
    <script src="../gitbook/theme.js"></script>
    
        
        <script src="../gitbook/gitbook-plugin-livereload/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-sharing/buttons.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

