
<!DOCTYPE HTML>
<html lang="" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>节省显存的微调推理技术对比 · GitBook</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.2.3">
        
        
        
    
    <link rel="stylesheet" href="../gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="../gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="大模型微调技术：fine-tune、parameter-efficient fine-tune和prompt-tune.html" />
    
    

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="Type to search" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="../">
            
                <a href="../">
            
                    
                    Introduction
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" data-path="../天工GPT调研/天工GPT调研.html">
            
                <a href="../天工GPT调研/天工GPT调研.html">
            
                    
                    天工GPT调研
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3" data-path="../相关GPT调研/相关GPT调研.html">
            
                <a href="../相关GPT调研/相关GPT调研.html">
            
                    
                    相关GPT调研
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4" >
            
                <span>
            
                    
                    相关技术调研
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.4.1" >
            
                <span>
            
                    
                    第一章：训练微调相关技术
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter active" data-level="1.4.1.1" data-path="微调范式对比.html">
            
                <a href="微调范式对比.html">
            
                    
                    节省显存的微调推理技术对比
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.1.2" data-path="大模型微调技术：fine-tune、parameter-efficient fine-tune和prompt-tune.html">
            
                <a href="大模型微调技术：fine-tune、parameter-efficient fine-tune和prompt-tune.html">
            
                    
                    LLM三种微调技术对比
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.1.3" data-path="Prompt-Tuning、Instruction-Tuning和Chain-of-Thought.html">
            
                <a href="Prompt-Tuning、Instruction-Tuning和Chain-of-Thought.html">
            
                    
                    LLM三种训练技术对比
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.1.4" data-path="思维链.html">
            
                <a href="思维链.html">
            
                    
                    思维链简介
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.4.2" >
            
                <span>
            
                    
                    第二章：定制化GPT
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.4.2.1" data-path="../Chapter1/根据自己数据库让GPT作答.html">
            
                <a href="../Chapter1/根据自己数据库让GPT作答.html">
            
                    
                    根据自己数据库让GPT作答
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.2.2" data-path="../Chapter1/训练部署自己的羊驼 Alpaca-LoRA.html">
            
                <a href="../Chapter1/训练部署自己的羊驼 Alpaca-LoRA.html">
            
                    
                    训练部署自己的羊驼 Alpaca-LoRA
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.2.3" data-path="../Chapter1/训练部署自己的ChatGLM-6B.html">
            
                <a href="../Chapter1/训练部署自己的ChatGLM-6B.html">
            
                    
                    训练部署自己的ChatGLM-6B
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.2.4" data-path="../Chapter1/Windows+6GB显卡版本和CPU版本的本地部署ChatGLM-6B.html">
            
                <a href="../Chapter1/Windows+6GB显卡版本和CPU版本的本地部署ChatGLM-6B.html">
            
                    
                    Windows+6GB显+CPU本地部署ChatGLM-6B.md
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.4.3" >
            
                <span>
            
                    
                    第三章：数据底座方法与调研
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.4.3.1" data-path="../Chapter3/Self-Instruct数据.html">
            
                <a href="../Chapter3/Self-Instruct数据.html">
            
                    
                    Self-Instruct自动生成微调数据
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.3.2" data-path="../Chapter3/学术问答数据集调研.html">
            
                <a href="../Chapter3/学术问答数据集调研.html">
            
                    
                    学术问答数据集调研
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.4.4" >
            
                <span>
            
                    
                    第四章：其他相关知识
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.4.4.1" data-path="../Chapter4/Lamini.html">
            
                <a href="../Chapter4/Lamini.html">
            
                    
                    Lamini LLM引擎学习
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.4.2" data-path="../Chapter4/GPT与知识图谱.html">
            
                <a href="../Chapter4/GPT与知识图谱.html">
            
                    
                    GPT与知识图谱
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.4.3" data-path="../Chapter4/Entailment Trees论文学习笔记.html">
            
                <a href="../Chapter4/Entailment Trees论文学习笔记.html">
            
                    
                    Entailment Trees论文学习笔记
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.4.4" data-path="../Chapter4/METGEN论文学习笔记.html">
            
                <a href="../Chapter4/METGEN论文学习笔记.html">
            
                    
                    METGEN论文学习笔记
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.5" data-path="../技术框架/技术框架.html">
            
                <a href="../技术框架/技术框架.html">
            
                    
                    毕设技术框架
            
                </a>
            

            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            Published with GitBook
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href=".." >节省显存的微调推理技术对比</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <h1 id="&#x5FAE;&#x8C03;&#x8303;&#x5F0F;&#x5BF9;&#x6BD4;">&#x5FAE;&#x8C03;&#x8303;&#x5F0F;&#x5BF9;&#x6BD4;</h1>
<p>&#x53C2;&#x8003;&#x94FE;&#x63A5;&#xFF1A;<a href="https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247616263&amp;idx=1&amp;sn=092a7e80ec9fa8bc87922c443177e97c&amp;chksm=96ebd887a19c519190cc432c28f076b180f83a29d1643c05c0fe98b47c3535838bf97cb5411b&amp;scene=21#wechat_redirect" target="_blank">&#x6709;&#x54EA;&#x4E9B;&#x7701;&#x5185;&#x5B58;&#x7684;&#x5927;&#x8BED;&#x8A00;&#x6A21;&#x578B;&#x8BAD;&#x7EC3;/&#x5FAE;&#x8C03;/&#x63A8;&#x7406;&#x65B9;&#x6CD5;&#xFF1F;</a></p>
<h5 id="&#x4E00;&#x3001;memory-efficient-&#x7684;-llms-&#x7684;&#x8BAD;&#x7EC3;&#x5FAE;&#x8C03;&#x63A8;&#x7406;&#x65B9;&#x6CD5;&#xFF0C;&#x5305;&#x62EC;&#xFF1A;">&#x4E00;&#x3001;Memory-Efficient &#x7684; LLMs &#x7684;&#x8BAD;&#x7EC3;/&#x5FAE;&#x8C03;/&#x63A8;&#x7406;&#x65B9;&#x6CD5;&#xFF0C;&#x5305;&#x62EC;&#xFF1A;</h5>
<pre><code>&#x25CF; fp16
&#x25CF; int8
&#x25CF; LoRA
&#x25CF; Gradient checkpointing
&#x25CF; Torch FSDP
&#x25CF; CPU offloading
</code></pre><h5 id="&#x4E8C;&#x3001;&#x4F30;&#x7B97;&#x6A21;&#x578B;&#x6240;&#x9700;&#x7684;ram&#xFF1A;&#x53C2;&#x6570;&#x4E2A;&#x6570;--&#x7CBE;&#x5EA6;--4--cuda&#x5185;&#x6838;13---hiddensizeintermediatesizecontextlengthnumhiddenlayers1024-g--batch&#x6570;">&#x4E8C;&#x3001;&#x4F30;&#x7B97;&#x6A21;&#x578B;&#x6240;&#x9700;&#x7684;RAM&#xFF1A;&#x53C2;&#x6570;&#x4E2A;&#x6570; <em> &#x7CBE;&#x5EA6; </em> 4 + CUDA&#x5185;&#x6838;1.3  + [(hidden_size+intermediate_size)<em>context_length\</em>num_hidden_layers]/1024 G * batch&#x6570;</h5>
<p>&#x6839;&#x636E;&#x53C2;&#x6570;&#x91CF;&#x4F30;&#x8BA1;&#x6A21;&#x578B;&#x5927;&#x81F4;&#x6240;&#x9700;&#x7684; RAM&#xFF0C;&#x8FDB;&#x800C;&#x901A;&#x8FC7;&#x4F30;&#x7B97;&#x8BBE;&#x7F6E; batch_size&#xFF0C;&#x8BBE;&#x7F6E;&#x6A21;&#x578B;&#x7CBE;&#x5EA6;&#xFF0C;&#x9009;&#x62E9;&#x5FAE;&#x8C03;&#x65B9;&#x6CD5;&#x548C;&#x53C2;&#x6570;&#x5206;&#x5E03;&#x65B9;&#x6CD5;&#x7B49;&#x3002;</p>
<p>&#xFF08;1&#xFF09;&#x7528; <strong>LLaMA-6B</strong> &#x6A21;&#x578B;&#x4E3A;&#x4F8B;&#x4F30;&#x7B97;&#x5176;&#x5927;&#x81F4;&#x9700;&#x8981;&#x7684;&#x5185;&#x5B58;&#xFF1A;</p>
<pre><code>1. &#x9996;&#x5148;&#x8003;&#x8651;&#x7CBE;&#x5EA6;&#x5BF9;&#x6240;&#x9700;&#x5185;&#x5B58;&#x7684;&#x5F71;&#x54CD;&#xFF1A;
&#x25CF; fp32 &#x7CBE;&#x5EA6;&#xFF0C;&#x4E00;&#x4E2A;&#x53C2;&#x6570;&#x9700;&#x8981; 32 bits, 4 bytes.
&#x25CF; fp16 &#x7CBE;&#x5EA6;&#xFF0C;&#x4E00;&#x4E2A;&#x53C2;&#x6570;&#x9700;&#x8981; 16 bits, 2 bytes.
&#x25CF; int8 &#x7CBE;&#x5EA6;&#xFF0C;&#x4E00;&#x4E2A;&#x53C2;&#x6570;&#x9700;&#x8981; 8 bits, 1 byte.
</code></pre><pre><code>2. &#x5176;&#x6B21;&#xFF0C;&#x8003;&#x8651;&#x6A21;&#x578B;&#x9700;&#x8981;&#x7684; RAM &#x5927;&#x81F4;&#x5206;&#x4E09;&#x4E2A;&#x90E8;&#x5206;&#xFF1A;
&#x25CF; &#x6A21;&#x578B;&#x53C2;&#x6570;&#xFF1A;&#x6A21;&#x578B;&#x53C2;&#x6570;&#xFF1A;&#x7B49;&#x4E8E;&#x53C2;&#x6570;&#x91CF;*&#x6BCF;&#x4E2A;&#x53C2;&#x6570;&#x6240;&#x9700;&#x5185;&#x5B58;&#x3002;
    &#x5BF9;&#x4E8E; fp32&#xFF0C;LLaMA-6B &#x9700;&#x8981; 6B*4 bytes = 24GB&#x5185;&#x5B58;
    &#x5BF9;&#x4E8E; int8&#xFF0C;LLaMA-6B &#x9700;&#x8981; 6B*1 byte = 6GB
&#x25CF; &#x68AF;&#x5EA6;&#xFF1A;&#x7B49;&#x4E8E;&#x53C2;&#x6570;&#x91CF;*&#x6BCF;&#x4E2A;&#x68AF;&#x5EA6;&#x53C2;&#x6570;&#x6240;&#x9700;&#x5185;&#x5B58;&#x3002;
&#x25CF; &#x4F18;&#x5316;&#x5668;&#x53C2;&#x6570;&#xFF1A;&#x4E0D;&#x540C;&#x7684;&#x4F18;&#x5316;&#x5668;&#x6240;&#x50A8;&#x5B58;&#x7684;&#x53C2;&#x6570;&#x91CF;&#x4E0D;&#x540C;&#x3002;&#x5BF9;&#x4E8E;&#x5E38;&#x7528;&#x7684; AdamW &#x6765;&#x8BF4;&#xFF0C;&#x9700;&#x8981;&#x50A8;&#x5B58;&#x4E24;&#x500D;&#x7684;&#x6A21;&#x578B;&#x53C2;&#x6570;    &#xFF08;&#x7528;&#x6765;&#x50A8;&#x5B58;&#x4E00;&#x9636;&#x548C;&#x4E8C;&#x9636;momentum&#xFF09;&#x3002;
    fp32 &#x7684; LLaMA-6B&#xFF0C;AdamW &#x9700;&#x8981; 6B*8 bytes = 48 GB
    int8 &#x7684; LLaMA-6B&#xFF0C;AdamW &#x9700;&#x8981; 6B*2 bytes = 12 GB
</code></pre><pre><code>3. CUDA kernel &#x4E5F;&#x4F1A;&#x5360;&#x636E;&#x4E00;&#x4E9B; RAM&#xFF0C;&#x5927;&#x6982; 1.3GB &#x5DE6;&#x53F3;
    torch.ones((1&#xFF0C;1)).to(&quot;cuda&quot;)
    print_gpu utilization()
    GPU memory occupied: 1343 MB
</code></pre><pre><code>4. &#x7EFC;&#x5408;1&#x3001;2&#x3001;3&#x6B65;&#x9AA4;&#xFF0C;int8 &#x7CBE;&#x5EA6;&#x7684; LLaMA-6B &#x6A21;&#x578B;&#x90E8;&#x5206;&#x5927;&#x81F4;&#x9700;&#x8981; 6GB+6GB+12GB+1.3GB = 25.3GB &#x5DE6;&#x53F3;&#x3002;
&#x518D;&#x6839;&#x636E;LLaMA&#x7684;&#x67B6;&#x6784;&#xFF08;hidden_size = 4096, intermediate_size =11008, num_hidden_layers = 32, context_length = 2048&#xFF09;&#x8BA1;&#x7B97;&#x4E2D;&#x95F4;&#x53D8;&#x91CF;&#x5185;&#x5B58;&#x3002;
</code></pre><p><img src="pics/&#x5FAE;&#x8C03;&#x8303;&#x5F0F;&#x5BF9;&#x6BD4;/image-20230504174211554.png" alt="image-20230504174211554" style="zoom:33%;"> </p>
<pre><code>5. &#x6240;&#x4EE5;&#x4E00;&#x5F20; A100&#xFF08;80GB RAM&#xFF09;&#x5927;&#x6982;&#x53EF;&#x4EE5;&#x5728; int8 &#x7CBE;&#x5EA6;&#xFF1B;batch_size = 50 &#x7684;&#x8BBE;&#x5B9A;&#x4E0B;&#x8FDB;&#x884C;&#x5168;&#x53C2;&#x6570;&#x8BAD;&#x7EC3;&#x3010;25.3+990/1024*50&#x3011;&#x3002;
</code></pre><hr>
<h5 id="&#x4E09;&#x3001;fp16-mixed-precision">&#x4E09;&#x3001;Fp16-mixed precision</h5>
<p><img src="pics/&#x5FAE;&#x8C03;&#x8303;&#x5F0F;&#x5BF9;&#x6BD4;/image-20230504174939772.png" alt="image-20230504174939772" style="zoom:33%;"> </p>
<ol>
<li>&#x5927;&#x81F4;&#x601D;&#x8DEF;&#xFF1A;&#x5728;&#x524D;&#x5411;&#x53CD;&#x9988; &#x548C; &#x68AF;&#x5EA6;&#x8BA1;&#x7B97; &#x7684;&#x65F6;&#x5019;&#x4F7F;&#x7528; fp16 &#x6765;&#x52A0;&#x901F;&#xFF0C;&#x4F46;&#x662F;&#x5728;&#x66F4;&#x65B0;&#x53C2;&#x6570;&#x65F6;&#x4F7F;&#x7528; fp32&#x3002;</li>
<li>torch&#x5B9E;&#x73B0;&#xFF1A;torch fp16 &#x63A8;&#x7406;&#x76F4;&#x63A5;&#x4F7F;&#x7528; model.half() &#x5C06;&#x6A21;&#x578B;&#x8F6C;&#x6362;&#x4E3A;fp16&#x3002;</li>
</ol>
<pre><code>model.half()
</code></pre><ol>
<li>&#x4F7F;&#x7528; Huggingface Transformers&#xFF1A;&#x5728; TrainingArguments &#x91CC;&#x58F0;&#x660E; fp16=True</li>
</ol>
<hr>
<h5 id="&#x56DB;&#x3001;int8-bitsandbytes&#x91CF;&#x5316;&#x65B9;&#x6CD5;">&#x56DB;&#x3001;Int8-bitsandbytes&#x91CF;&#x5316;&#x65B9;&#x6CD5;</h5>
<p>&#x53C2;&#x8003;&#x8BBA;&#x6587;&#xFF1A;<strong>LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale</strong><a href="https://arxiv.org/abs/2208.07339*" target="_blank">https://arxiv.org/abs/2208.07339*</a></p>
<ol>
<li><p>Int8 &#x662F;&#x4E2A;&#x5F88;&#x6781;&#x7AEF;&#x7684;&#x6570;&#x636E;&#x7C7B;&#x578B;&#xFF0C;&#x5B83;&#x6700;&#x591A;&#x53EA;&#x80FD;&#x8868;&#x793A; - 128&#xFF5E;127 &#x7684;&#x6570;&#x5B57;&#xFF0C;&#x5E76;&#x4E14;&#x5B8C;&#x5168;&#x6CA1;&#x6709;&#x7CBE;&#x5EA6;&#x3002;</p>
</li>
<li><p>&#x4E3A;&#x4E86;&#x5728;&#x8BAD;&#x7EC3;&#x548C; inference &#x4E2D;&#x4F7F;&#x7528;&#x8FD9;&#x4E2A;&#x6570;&#x636E;&#x7C7B;&#x578B;&#xFF0C;bitsandbytes &#x4F7F;&#x7528;&#x4E86;&#x4E24;&#x4E2A;&#x65B9;&#x6CD5;&#x6700;&#x5927;&#x7A0B;&#x5EA6;&#x5730;&#x964D;&#x4F4E;&#x4E86;&#x5176;&#x5E26;&#x6765;&#x7684;&#x8BEF;&#x5DEE;&#xFF1A;
<a href="https://huggingface.co/blog/hf-bitsandbytes-integration" target="_blank">A Gentle Introduction to 8-bit Matrix Multiplication for transformers at scale using transformers, accelerate and bitsandbytes (huggingface.co)</a></p>
<p>&#xFF08;1&#xFF09;vector-wise quantization&#xFF1A;&#x77E2;&#x91CF;&#x91CF;&#x5316;</p>
<p>&#xFF08;2&#xFF09;mixed precision decompasition&#xFF1A;&#x6DF7;&#x5408;&#x7CBE;&#x5EA6;&#x53CD;&#x7F16;&#x8BD1;</p>
<p><img src="https://huggingface.co/blog/assets/96_hf_bitsandbytes_integration/Mixed-int8.gif" alt="&#x6DF7;&#x5408;&#x56FD;&#x9645;8.gif"> </p>
</li>
<li><p>&#x501F;&#x52A9; Huggingface PEFT&#xFF0C;&#x4F7F;&#x7528; int8 &#x8BAD;&#x7EC3; opt-6.5B &#x7684;&#x5B8C;&#x6574;&#x6D41;&#x7A0B;&#xFF1A;</p>
<p><em><a href="https://github.com/huggingface/peft/blob/main/examples/int8_training/Finetune_opt_bnb_peft.ipynb" target="_blank">https://github.com/huggingface/peft/blob/main/examples/int8_training/Finetune_opt_bnb_peft.ipynb</a></em></p>
</li>
</ol>
<hr>
<h5 id="&#x4E94;&#x3001;lora&#xFF1A;low-rank-adaptation-&#x662F;&#x5FAE;&#x8C03;-llms-&#x6700;&#x5E38;&#x7528;&#x7684;&#x7701;&#x5185;&#x5B58;&#x65B9;&#x6CD5;&#x4E4B;&#x4E00;&#x3010;&#x4F4E;&#x79E9;&#x8F6C;&#x6362;&#x3011;">&#x4E94;&#x3001;LoRA&#xFF1A;Low-Rank Adaptation &#x662F;&#x5FAE;&#x8C03; LLMs &#x6700;&#x5E38;&#x7528;&#x7684;&#x7701;&#x5185;&#x5B58;&#x65B9;&#x6CD5;&#x4E4B;&#x4E00;&#x3010;&#x4F4E;&#x79E9;&#x8F6C;&#x6362;&#x3011;</h5>
<p><em><a href="https://arxiv.org/pdf/2106.09685.pdf" target="_blank">https://arxiv.org/pdf/2106.09685.pdf</a></em></p>
<p><img src="pics/&#x5FAE;&#x8C03;&#x8303;&#x5F0F;&#x5BF9;&#x6BD4;/image-20230504175445174.png" alt="image-20230504175445174" style="zoom:33%;"> </p>
<p><strong>&#x7B80;&#x8FF0;&#x539F;&#x7406;&#xFF1A;</strong>&#x5FAE;&#x8C03;LLM&#x65F6;&#xFF0C;&#x66F4;&#x65B0;&#x77E9;&#x9635;&#x5F80;&#x5F80;&#x662F;&#x4F4E;&#x79E9;&#x77E9;&#x9635;&#x3002;&#xFF08;&#x4F4E;&#x79E9;&#x77E9;&#x9635;&#x662F;&#x6307;&#x77E9;&#x9635;&#x4E2D;&#x5305;&#x542B;&#x7684;&#x4FE1;&#x606F;&#x53EF;&#x4EE5;&#x7528;&#x8F83;&#x4F4E;&#x7EF4;&#x5EA6;&#x7684;&#x5B50;&#x7A7A;&#x95F4;&#x8FDB;&#x884C;&#x8FD1;&#x4F3C;&#x8868;&#x793A;&#x7684;&#x77E9;&#x9635;&#xFF0C;&#x7531;&#x4E8E;&#x4F4E;&#x79E9;&#x77E9;&#x9635;&#x5728;&#x50A8;&#x5B58;&#x548C;&#x8BA1;&#x7B97;&#x65B9;&#x9762;&#x90FD;&#x6709;&#x4F18;&#x52BF;&#xFF0C;&#x6240;&#x4EE5;&#x5B83;&#x4EEC;&#x88AB;&#x5E7F;&#x6CDB;&#x5E94;&#x7528;&#x4E8E;&#x538B;&#x7F29;&#x3001;&#x964D;&#x566A;&#x3001;&#x7279;&#x5F81;&#x63D0;&#x53D6;&#x3001;&#x6A21;&#x578B;&#x538B;&#x7F29;&#x7B49;&#x4EFB;&#x52A1;&#xFF09;&#xFF08;&#x77E9;&#x9635;&#x7684;&#x79E9;&#x662F;&#x6307;&#x77E9;&#x9635;&#x4E2D;&#x7EBF;&#x6027;&#x65E0;&#x5173;&#x7684;&#x884C;&#x6216;&#x5217;&#x7684;&#x6700;&#x5927;&#x6570;&#x76EE;&#xFF09;</p>
<p><img src="pics/&#x5FAE;&#x8C03;&#x8303;&#x5F0F;&#x5BF9;&#x6BD4;/image-20230504180424619.png" alt="image-20230504180424619" style="zoom:33%;"> </p>
<p>&#x6240;&#x4EE5;LoRA &#x7684;&#x4F5C;&#x8005;&#x6839;&#x636E;&#x8FD9;&#x4E00;&#x7279;&#x70B9;&#x5C06;&#x66F4;&#x65B0;&#x77E9;&#x9635;&#x53D8;&#x6210;&#x4E24;&#x4E2A;&#x4F4E;&#x79E9;&#x77E9;&#x9635;&#x7684;&#x79EF;&#x79EF;B . A &#x3002;</p>
<p><img src="pics/&#x5FAE;&#x8C03;&#x8303;&#x5F0F;&#x5BF9;&#x6BD4;/image-20230504180602035.png" alt="image-20230504180602035" style="zoom:53%;"> </p>
<p>&#x4F8B;&#x5B50;&#xFF1A;&#x501F;&#x52A9; Huggingface PEFT &#x6846;&#x67B6;&#xFF0C;&#x4F7F;&#x7528; LoRA &#x5FAE;&#x8C03; mt0&#xFF1A;</p>
<p><em><a href="https://github.com/huggingface/peft/blob/main/examples/conditional_generation/peft_lora_seq2seq.ipynb" target="_blank">https://github.com/huggingface/peft/blob/main/examples/conditional_generation/peft_lora_seq2seq.ipynb</a></em></p>
<hr>
<h5 id="&#x516D;&#x3001;gradient-checkpointing">&#x516D;&#x3001;Gradient Checkpointing</h5>
<p>&#x5728; torch &#x4E2D;&#x4F7F;&#x7528; - &#x628A; model &#x7528;&#x4E00;&#x4E2A; customize &#x7684; function &#x5305;&#x88C5;&#x4E00;&#x4E0B;&#x5373;&#x53EF;&#xFF0C;&#x8BE6;&#x89C1;&#xFF1A;</p>
<p><strong>Explore Gradient-Checkpointing in PyTorch</strong></p>
<p><em><a href="https://qywu.github.io/2019/05/22/explore-gradient-checkpointing.html" target="_blank">https://qywu.github.io/2019/05/22/explore-gradient-checkpointing.html</a></em></p>
<p>&#x5728; Huggingface Transformers &#x4E2D;&#x4F7F;&#x7528;&#xFF1A;</p>
<p><em><a href="https://huggingface.co/docs/transformers/v4.27.2/en/perf_train_gpu_one#gradient-checkpointing" target="_blank">https://huggingface.co/docs/transformers/v4.27.2/en/perf_train_gpu_one#gradient-checkpointing</a></em></p>
<hr>
<h5 id="&#x4E03;&#x3001;torch-fsdpcpu-offload&#x3010;&#x5206;&#x5E03;&#x5F0F;&#x3011;">&#x4E03;&#x3001;Torch FSDP+CPU offload&#x3010;&#x5206;&#x5E03;&#x5F0F;&#x3011;</h5>
<ol>
<li><p><strong>&#x539F;&#x7406;&#xFF1A;</strong>Fully Sharded Data Paralle&#xFF08;FSDP&#xFF09;&#x548C; DeepSpeed &#x7C7B;&#x4F3C;&#xFF0C;&#x5747;&#x901A;&#x8FC7; ZeRO &#x7B49;&#x5206;&#x5E03;&#x4F18;&#x5316;&#x7B97;&#x6CD5;&#xFF0C;&#x51CF;&#x5C11;&#x5185;&#x5B58;&#x7684;&#x5360;&#x7528;&#x91CF;&#x3002;&#x5176;&#x5C06;&#x6A21;&#x578B;&#x53C2;&#x6570;&#xFF0C;&#x68AF;&#x5EA6;&#x548C;&#x4F18;&#x5316;&#x5668;&#x72B6;&#x6001;&#x5206;&#x5E03;&#x81F3;&#x591A;&#x4E2A; GPU &#x4E0A;&#xFF0C;&#x800C;&#x975E;&#x50CF; DDP &#x4E00;&#x6837;&#xFF0C;&#x5728;&#x6BCF;&#x4E2A; GPU &#x4E0A;&#x4FDD;&#x7559;&#x5B8C;&#x6574;&#x526F;&#x672C;&#x3002;</p>
<p>CPU offload &#x5219;&#x5141;&#x8BB8;&#x5728;&#x4E00;&#x4E2A; back propagation &#x4E2D;&#xFF0C;&#x5C06;&#x53C2;&#x6570;&#x52A8;&#x6001;&#x5730;&#x4ECE; GPU -&gt; CPU, CPU -&gt; GPU &#x8FDB;&#x884C;&#x8F6C;&#x79FB;&#xFF0C;&#x4ECE;&#x800C;&#x8282;&#x7701; GPU &#x5185;&#x5B58;&#x3002;</p>
</li>
<li><p>Huggingface &#x8FD9;&#x7BC7;&#x535A;&#x6587;&#x89E3;&#x91CA;&#x4E86; ZeRO &#x7684;&#x5927;&#x81F4;&#x5B9E;&#x73B0;&#x65B9;&#x6CD5;&#xFF1A;</p>
</li>
<li><p><a href="https://huggingface.co/blog/zero-deepspeed-fairscale" target="_blank">https://huggingface.co/blog/zero-deepspeed-fairscale</a></p>
</li>
</ol>
<pre><code>ZeRO&#x96F6;&#x5197;&#x4F59;&#x4F18;&#x5316;&#x5668; &#x7684;&#x5DE7;&#x5999;&#x65B9;&#x6CD5;&#x662F;&#x5728;&#x6240;&#x6709; GPU &#x4E0A;&#x5E73;&#x5747;&#x5212;&#x5206;&#x53C2;&#x6570;&#x3001;&#x68AF;&#x5EA6;&#x548C;&#x4F18;&#x5316;&#x5668;&#x72B6;&#x6001;&#xFF0C;&#x5E76;&#x4E3A;&#x6BCF;&#x4E2A; GPU &#x63D0;&#x4F9B;&#x4E00;&#x4E2A;&#x5206;&#x533A;&#xFF08;&#x4E5F;&#x79F0;&#x4E3A;&#x5206;&#x7247;&#xFF09;&#x3002;&#x8FD9;&#x5BFC;&#x81F4; GPU &#x4E4B;&#x95F4;&#x7684;&#x6570;&#x636E;&#x5B58;&#x50A8;&#x96F6;&#x91CD;&#x53E0;&#x3002;&#x5728;&#x8FD0;&#x884C;&#x65F6;&#xFF0C;&#x6BCF;&#x4E2A; GPU &#x901A;&#x8FC7;&#x8981;&#x6C42;&#x53C2;&#x4E0E;&#x7684; GPU &#x53D1;&#x9001;&#x5B83;&#x6240;&#x7F3A;&#x5C11;&#x7684;&#x4FE1;&#x606F;&#x6765;&#x52A8;&#x6001;&#x6784;&#x5EFA;&#x6BCF;&#x4E00;&#x5C42;&#x7684;&#x6570;&#x636E;&#x3002;
FairScale &#x548C; DeepSpeed &#x4EC5;&#x5BF9;&#x4F18;&#x5316;&#x5668;&#x72B6;&#x6001;&#x548C;&#x68AF;&#x5EA6;&#x6267;&#x884C;&#x5206;&#x533A;&#xFF08;&#x5206;&#x7247;&#xFF09;&#x3002;&#x6A21;&#x578B;&#x53C2;&#x6570;&#x5206;&#x7247;&#x5E94;&#x8BE5;&#x5F88;&#x5FEB;&#x5C31;&#x4F1A;&#x5728;DeepSpeed&#x548C;FairScale&#x4E2D;&#x63A8;&#x51FA;&#x3002;
ZeRO &#x5378;&#x8F7D;&#xFF08;CPU&#xFF09;&#x3002;&#x6B64;&#x529F;&#x80FD;&#x5C06;&#x4E00;&#x4E9B;&#x5904;&#x7406;&#x548C;&#x5185;&#x5B58;&#x9700;&#x6C42;&#x5378;&#x8F7D;&#x5230;&#x4E3B;&#x673A;&#x7684; CPU&#xFF0C;&#x4ECE;&#x800C;&#x5141;&#x8BB8;&#x66F4;&#x591A;&#x5185;&#x5BB9;&#x9002;&#x5E94; GPU&#x3002;
</code></pre><ol>
<li><p>&#x501F;&#x52A9; torch &#x5B9E;&#x73B0; FSDP&#xFF0C;&#x53EA;&#x9700;&#x8981;&#x5C06; model &#x7528; FSDPwarp &#x4E00;&#x4E0B;&#xFF1B;&#x540C;&#x6837;&#xFF0C;cpu_offload &#x4E5F;&#x53EA;&#x9700;&#x8981;&#x4E00;&#x884C;&#x4EE3;&#x7801;&#xFF1A;</p>
</li>
<li><p><a href="https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/" target="_blank">https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/</a></p>
</li>
<li><p>&#x5728;&#x8FD9;&#x4E2A;&#x53EF;&#x4EE5;&#x67E5;&#x770B; FSDP &#x652F;&#x6301;&#x7684;&#x6A21;&#x578B;&#xFF1A;</p>
</li>
<li><p><a href="https://pytorch.org/docs/stable/fsdp.html" target="_blank">https://pytorch.org/docs/stable/fsdp.html</a></p>
</li>
<li><p>&#x5728; Huggingface Transformers &#x4E2D;&#x4F7F;&#x7528; Torch FSDP&#xFF1A;</p>
</li>
<li><p><a href="https://huggingface.co/docs/transformers/v4.27.2/en/main_classes/trainer#transformers.Trainin" target="_blank">https://huggingface.co/docs/transformers/v4.27.2/en/main_classes/trainer#transformers.Trainin</a></p>
</li>
</ol>

                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                
                <a href="大模型微调技术：fine-tune、parameter-efficient fine-tune和prompt-tune.html" class="navigation navigation-next navigation-unique" aria-label="Next page: LLM三种微调技术对比">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"节省显存的微调推理技术对比","level":"1.4.1.1","depth":3,"next":{"title":"LLM三种微调技术对比","level":"1.4.1.2","depth":3,"path":"Chapter2/大模型微调技术：fine-tune、parameter-efficient fine-tune和prompt-tune.md","ref":"Chapter2/大模型微调技术：fine-tune、parameter-efficient fine-tune和prompt-tune.md","articles":[]},"previous":{"title":"第一章：训练微调相关技术","level":"1.4.1","depth":2,"ref":"","articles":[{"title":"节省显存的微调推理技术对比","level":"1.4.1.1","depth":3,"path":"Chapter2/微调范式对比.md","ref":"Chapter2/微调范式对比.md","articles":[]},{"title":"LLM三种微调技术对比","level":"1.4.1.2","depth":3,"path":"Chapter2/大模型微调技术：fine-tune、parameter-efficient fine-tune和prompt-tune.md","ref":"Chapter2/大模型微调技术：fine-tune、parameter-efficient fine-tune和prompt-tune.md","articles":[]},{"title":"LLM三种训练技术对比","level":"1.4.1.3","depth":3,"path":"Chapter2/Prompt-Tuning、Instruction-Tuning和Chain-of-Thought.md","ref":"Chapter2/Prompt-Tuning、Instruction-Tuning和Chain-of-Thought.md","articles":[]},{"title":"思维链简介","level":"1.4.1.4","depth":3,"path":"Chapter2/思维链.md","ref":"Chapter2/思维链.md","articles":[]}]},"dir":"ltr"},"config":{"gitbook":"*","theme":"default","variables":{},"plugins":["livereload"],"pluginsConfig":{"livereload":{},"highlight":{},"search":{},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"sharing":{"facebook":true,"twitter":true,"google":false,"weibo":false,"instapaper":false,"vk":false,"all":["facebook","google","twitter","weibo","instapaper"]},"fontsettings":{"theme":"white","family":"sans","size":2},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"}},"file":{"path":"Chapter2/微调范式对比.md","mtime":"2023-05-09T05:22:30.026Z","type":"markdown"},"gitbook":{"version":"3.2.3","time":"2023-06-01T03:46:06.651Z"},"basePath":"..","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="../gitbook/gitbook.js"></script>
    <script src="../gitbook/theme.js"></script>
    
        
        <script src="../gitbook/gitbook-plugin-livereload/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-sharing/buttons.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

